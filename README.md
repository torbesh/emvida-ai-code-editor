# EMVIDA AI Code Editor

## √úbersicht

Der EMVIDA AI Code Editor ist ein leistungsstarker Editor mit integrierten KI-Funktionen, der Entwicklern hilft, effizienter zu programmieren. Diese Version enth√§lt Integrationen f√ºr mehrere KI-Modelle, darunter Ollama, Mistral, Grok und Qwen.

## Neue KI-Integrationen

Der Editor wurde mit folgenden KI-Modell-Integrationen erweitert:

Hier ist die aktualisierte Anleitung f√ºr dein GitHub-Projekt [emvida-ai-code-editor](https://github.com/torbesh/emvida-ai-code-editor), angepasst an die neuesten Informationen zu LM Studio:


## üß† Einrichtung mit LM Studio

### 1. LM Studio herunterladen und installieren

* Besuche die offizielle Website: [lmstudio.ai](https://lmstudio.ai/downloads)
* Lade die passende Version f√ºr dein Betriebssystem herunter (Windows, macOS oder Linux).
* F√ºhre die Installation gem√§√ü den Anweisungen durch.

### 2. Sprachmodell herunterladen

* Starte LM Studio.
* Navigiere zum Tab **Discover** (Lupensymbol).
* Suche nach einem geeigneten Modell (z.‚ÄØB. *Meta-Llama-3.1-8B-Instruct* oder *Phi-3*).
* W√§hle ein Modell mit passender Quantisierung (z.‚ÄØB. Q4\_0 f√ºr 4-Bit) entsprechend deiner Hardware aus und lade es herunter.

### 3. Modell laden

* Wechsle zum **Chat**-Tab (Sprechblasensymbol).
* W√§hle im Dropdown-Men√º oben das heruntergeladene Modell aus.
* Das Modell wird nun geladen und ist einsatzbereit.

### 4. Lokalen Server aktivieren

* Gehe zum Tab **Developer**.
* Aktiviere den Schalter **Server aktivieren**.
* Der Server ist nun unter `http://localhost:1234/v1` erreichbar.

> **Hinweis:** Standardm√§√üig erfordert der lokale Server von LM Studio keine Authentifizierung oder Passwort. Die Verbindung erfolgt in der Regel √ºber die OpenAI-kompatible API unter `http://localhost:1234/v1`.&#x20;

---

## üíª Projekt verwenden

1. Lade die Datei [`EMVIDA_AI_CODE_EDITOR_OFFLINE.zip`](https://github.com/torbesh/emvida-ai-code-editor/blob/main/EMVIDA_AI_CODE_EDITOR_OFFLINE.zip) herunter.
2. Entpacke die ZIP-Datei in ein Verzeichnis deiner Wahl.
3. √ñffne die Datei `index.html` in deinem bevorzugten Webbrowser.

Stelle sicher, dass der lokale Server von LM Studio aktiv ist, bevor du `index.html` √∂ffnest, damit der Code-Editor korrekt mit dem Sprachmodell kommunizieren kann.


### Ollama Integration
- Lokale Ausf√ºhrung von KI-Modellen √ºber Ollama
- Standard-Endpunkt: http://localhost:11434/api
- Unterst√ºtzt CodeLlama, Llama 3, Mistral und andere Modelle
- Keine API-Schl√ºssel erforderlich

### Mistral Integration
- Verbindung zur Mistral AI API
- Unterst√ºtzt Mistral Large, Medium und Small Modelle
- Erfordert einen API-Schl√ºssel

### Grok Integration
- Verbindung zur Grok AI API
- Unterst√ºtzt das Grok-1 Modell
- Erfordert einen API-Schl√ºssel

### Qwen Integration
- Verbindung zur Qwen AI API (Alibaba Cloud)
- Unterst√ºtzt Qwen Max, Plus und Turbo Modelle
- Erfordert einen API-Schl√ºssel

## Verwendung

1. W√§hlen Sie ein KI-Modell aus dem Dropdown-Men√º
2. Geben Sie bei Bedarf Ihren API-Schl√ºssel ein
3. Stellen Sie die Temperatur und maximale Token-Anzahl ein
4. Geben Sie Ihre Anweisungen f√ºr die KI ein
5. Klicken Sie auf "Generate", um Code zu erzeugen oder zu bearbeiten

## Konfiguration

F√ºr lokale Modelle (Ollama):
- Stellen Sie sicher, dass Ollama auf Ihrem System installiert und ausgef√ºhrt wird
- Die Standard-Einstellung verwendet http://localhost:11434/api
- Sie k√∂nnen den Endpunkt und das Modell in den Einstellungen √§ndern

F√ºr Cloud-basierte Modelle (Mistral, Grok, Qwen):
- Ein g√ºltiger API-Schl√ºssel ist erforderlich
- Geben Sie den API-Schl√ºssel im entsprechenden Feld ein
- Sie k√∂nnen das zu verwendende Modell in den Einstellungen ausw√§hlen

## Funktionen

Alle KI-Integrationen unterst√ºtzen folgende Funktionen:
- Code-Vervollst√§ndigung
- Code-Chat (Fragen zum Code stellen)
- Code-Bearbeitung basierend auf Anweisungen
- Code-Refaktorierung
- Code-Erkl√§rung
- Code-Optimierung
- Test-Generierung
- Fehlerkorrektur
- Code-Dokumentation
- Vorschl√§ge f√ºr Design-Patterns

## Offline-Modus

Wenn keine Verbindung zu einem KI-Modell hergestellt werden kann, wechselt der Editor automatisch in den Offline-Modus mit vordefinierten Antworten.
